{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"User Guide Quick links: \ud83d\udcbb GitHub Repository \ud83d\udcda Documentation / Readthedocs \ud83d\udc0d PyPi project \ud83e\uddea Colab Demo / Kaggle Demo Quickstart Install the library with pip: pip install dl-translate To translate some text: import dl_translate as dlt mt = dlt . TranslationModel () # Slow when you load it for the first time text_hi = \"\u0938\u0902\u092f\u0941\u0915\u094d\u0924 \u0930\u093e\u0937\u094d\u091f\u094d\u0930 \u0915\u0947 \u092a\u094d\u0930\u092e\u0941\u0916 \u0915\u093e \u0915\u0939\u0928\u093e \u0939\u0948 \u0915\u093f \u0938\u0940\u0930\u093f\u092f\u093e \u092e\u0947\u0902 \u0915\u094b\u0908 \u0938\u0948\u0928\u094d\u092f \u0938\u092e\u093e\u0927\u093e\u0928 \u0928\u0939\u0940\u0902 \u0939\u0948\" mt . translate ( text_hi , source = dlt . lang . HINDI , target = dlt . lang . ENGLISH ) Above, you can see that dlt.lang contains variables representing each of the 50 available languages with auto-complete support. Alternatively, you can specify the language (e.g. \"Arabic\") or the language code (e.g. \"fr_XX\" for French): text_ar = \"\u0627\u0644\u0623\u0645\u064a\u0646 \u0627\u0644\u0639\u0627\u0645 \u0644\u0644\u0623\u0645\u0645 \u0627\u0644\u0645\u062a\u062d\u062f\u0629 \u064a\u0642\u0648\u0644 \u0625\u0646\u0647 \u0644\u0627 \u064a\u0648\u062c\u062f \u062d\u0644 \u0639\u0633\u0643\u0631\u064a \u0641\u064a \u0633\u0648\u0631\u064a\u0627.\" mt . translate ( text_ar , source = \"Arabic\" , target = \"fr_XX\" ) If you want to verify whether a language is available, you can check it: print ( mt . available_languages ()) # All languages that you can use print ( mt . available_codes ()) # Code corresponding to each language accepted print ( mt . get_lang_code_map ()) # Dictionary of lang -> code Usage Selecting a device When you load the model, you can specify the device: mt = dlt . TranslationModel ( device = \"auto\" ) By default, the value will be device=\"auto\" , which means it will use a GPU if possible. You can also explicitly set device=\"cpu\" or device=\"gpu\" , or some other strings accepted by torch.device() . In general, it is recommend to use a GPU if you want a reasonable processing time. Loading from a path By default, dlt.TranslationModel will download the model from the huggingface repo and cache it. However, you are free to load from a path: mt = dlt . TranslationModel ( \"/path/to/your/model/directory/\" ) Make sure that your tokenizer is also stored in the same directory if you use this approach. Using a different model You can also choose another model that has a similar format , e.g. mt = dlt . TranslationModel ( \"facebook/mbart-large-50-one-to-many-mmt\" ) Note that the available languages will change if you do this, so you will not be able to leverage dlt.lang or dlt.utils . Breaking down into sentences It is not recommended to use extremely long texts as it takes more time to process. Instead, you can try to break them down into sentences with the help of nltk . First install the library with pip install nltk , then run: import nltk nltk . download ( \"punkt\" ) text = \"Mr. Smith went to his favorite cafe. There, he met his friend Dr. Doe.\" sents = nltk . tokenize . sent_tokenize ( text , \"english\" ) # don't use dlt.lang.ENGLISH \" \" . join ( mt . translate ( sents , source = dlt . lang . ENGLISH , target = dlt . lang . FRENCH )) Batch size and verbosity when using translate It's possible to set a batch size (i.e. the number of elements processed at once) for mt.translate and whether you want to see the progress bar or not: ... mt = dlt . TranslationModel () mt . translate ( text , source , target , batch_size = 32 , verbose = True ) If you set batch_size=None , it will compute the entire text at once rather than splitting into \"chunks\". We recommend lowering batch_size if you do not have a lot of RAM or VRAM and run into CUDA memory error. Set a higher value if you are using a high-end GPU and the VRAM is not fully utilized. dlt.utils module An alternative to mt.available_languages() is the dlt.utils module. You can use it to find out which languages and codes are available: print ( dlt . utils . available_languages ( 'mbart50' )) # All languages that you can use print ( dlt . utils . available_codes ( 'mbart50' )) # Code corresponding to each language accepted print ( dlt . utils . get_lang_code_map ( 'mbart50' )) # Dictionary of lang -> code Advanced The following section assumes you have knowledge of PyTorch and Huggingface Transformers. Saving and loading If you wish to accelerate the loading time the translation model, you can use save_obj : mt = dlt . TranslationModel () mt . save_obj ( 'saved_model' ) # ... Then later you can reload it with load_obj : mt = dlt . TranslationModel . load_obj ( 'saved_model' ) # ... Warning: Only use this if you are certain the torch module saved in saved_model/weights.pt can be correctly loaded. Indeed, it is possible that the huggingface , torch or some other dependencies change between when you called save_obj and load_obj , and that might break your code. Thus, it is recommend to only run load_obj in the same environment/session as save_obj . Note this method might be deprecated in the future once there's no speed benefit in loading this way. Interacting with underlying model and tokenizer When initializing model , you can pass in arguments for the underlying BART model and tokenizer (which will respectively be passed to MBartForConditionalGeneration.from_pretrained and MBart50TokenizerFast.from_pretrained ): mt = dlt . TranslationModel ( model_options = dict ( state_dict =... , cache_dir =... , ... ), tokenizer_options = dict ( tokenizer_file =... , eos_token =... , ... ) ) You can also access the underlying transformers model and tokenizer : bart = mt . get_transformers_model () tokenizer = mt . get_tokenizer () See the huggingface docs for more information. bart_model.generate() keyword arguments When running mt.translate , you can also give a generation_options dictionary that is passed as keyword arguments to the underlying bart_model.generate() method: mt . translate ( text , source = dlt . lang . GERMAN , target = dlt . lang . SPANISH , generation_options = dict ( num_beams = 5 , max_length =... ) ) Learn more in the huggingface docs . Acknowledgement dl-translate is built on top of Huggingface's implementation of multilingual BART finetuned on many-to-many translation of over 50 languages, which is documented here . The original paper was written by Tang et. al from Facebook AI Research; you can find it here and cite it using the following: @article{tang2020multilingual, title={Multilingual translation with extensible multilingual pretraining and finetuning}, author={Tang, Yuqing and Tran, Chau and Li, Xian and Chen, Peng-Jen and Goyal, Naman and Chaudhary, Vishrav and Gu, Jiatao and Fan, Angela}, journal={arXiv preprint arXiv:2008.00401}, year={2020} } dlt is a wrapper with useful utils to save you time. For huggingface's transformers , the following snippet is shown as an example: from transformers import MBartForConditionalGeneration , MBart50TokenizerFast article_hi = \"\u0938\u0902\u092f\u0941\u0915\u094d\u0924 \u0930\u093e\u0937\u094d\u091f\u094d\u0930 \u0915\u0947 \u092a\u094d\u0930\u092e\u0941\u0916 \u0915\u093e \u0915\u0939\u0928\u093e \u0939\u0948 \u0915\u093f \u0938\u0940\u0930\u093f\u092f\u093e \u092e\u0947\u0902 \u0915\u094b\u0908 \u0938\u0948\u0928\u094d\u092f \u0938\u092e\u093e\u0927\u093e\u0928 \u0928\u0939\u0940\u0902 \u0939\u0948\" article_ar = \"\u0627\u0644\u0623\u0645\u064a\u0646 \u0627\u0644\u0639\u0627\u0645 \u0644\u0644\u0623\u0645\u0645 \u0627\u0644\u0645\u062a\u062d\u062f\u0629 \u064a\u0642\u0648\u0644 \u0625\u0646\u0647 \u0644\u0627 \u064a\u0648\u062c\u062f \u062d\u0644 \u0639\u0633\u0643\u0631\u064a \u0641\u064a \u0633\u0648\u0631\u064a\u0627.\" model = MBartForConditionalGeneration . from_pretrained ( \"facebook/mbart-large-50-many-to-many-mmt\" ) tokenizer = MBart50TokenizerFast . from_pretrained ( \"facebook/mbart-large-50-many-to-many-mmt\" ) # translate Hindi to French tokenizer . src_lang = \"hi_IN\" encoded_hi = tokenizer ( article_hi , return_tensors = \"pt\" ) generated_tokens = model . generate ( ** encoded_hi , forced_bos_token_id = tokenizer . lang_code_to_id [ \"fr_XX\" ]) tokenizer . batch_decode ( generated_tokens , skip_special_tokens = True ) # => \"Le chef de l 'ONU affirme qu 'il n 'y a pas de solution militaire en Syria.\" # translate Arabic to English tokenizer . src_lang = \"ar_AR\" encoded_ar = tokenizer ( article_ar , return_tensors = \"pt\" ) generated_tokens = model . generate ( ** encoded_ar , forced_bos_token_id = tokenizer . lang_code_to_id [ \"en_XX\" ]) tokenizer . batch_decode ( generated_tokens , skip_special_tokens = True ) # => \"The Secretary-General of the United Nations says there is no military solution in Syria.\" With dlt , you can run: import dl_translate as dlt article_hi = \"\u0938\u0902\u092f\u0941\u0915\u094d\u0924 \u0930\u093e\u0937\u094d\u091f\u094d\u0930 \u0915\u0947 \u092a\u094d\u0930\u092e\u0941\u0916 \u0915\u093e \u0915\u0939\u0928\u093e \u0939\u0948 \u0915\u093f \u0938\u0940\u0930\u093f\u092f\u093e \u092e\u0947\u0902 \u0915\u094b\u0908 \u0938\u0948\u0928\u094d\u092f \u0938\u092e\u093e\u0927\u093e\u0928 \u0928\u0939\u0940\u0902 \u0939\u0948\" article_ar = \"\u0627\u0644\u0623\u0645\u064a\u0646 \u0627\u0644\u0639\u0627\u0645 \u0644\u0644\u0623\u0645\u0645 \u0627\u0644\u0645\u062a\u062d\u062f\u0629 \u064a\u0642\u0648\u0644 \u0625\u0646\u0647 \u0644\u0627 \u064a\u0648\u062c\u062f \u062d\u0644 \u0639\u0633\u0643\u0631\u064a \u0641\u064a \u0633\u0648\u0631\u064a\u0627.\" mt = dlt . TranslationModel () translated_fr = mt . translate ( article_hi , source = dlt . lang . HINDI , target = dlt . lang . FRENCH ) translated_en = mt . translate ( article_ar , source = dlt . lang . ARABIC , target = dlt . lang . ENGLISH ) Notice you don't have to think about tokenizers, condition generation, pretrained models, and regional codes; you can just tell the model what to translate! If you are experienced with huggingface 's ecosystem, then you should be familiar enough with the example above that you wouldn't need this library. However, if you've never heard of huggingface or mBART, then I hope using this library will give you enough motivation to learn more about them :)","title":"User Guide"},{"location":"#user-guide","text":"Quick links: \ud83d\udcbb GitHub Repository \ud83d\udcda Documentation / Readthedocs \ud83d\udc0d PyPi project \ud83e\uddea Colab Demo / Kaggle Demo","title":"User Guide"},{"location":"#quickstart","text":"Install the library with pip: pip install dl-translate To translate some text: import dl_translate as dlt mt = dlt . TranslationModel () # Slow when you load it for the first time text_hi = \"\u0938\u0902\u092f\u0941\u0915\u094d\u0924 \u0930\u093e\u0937\u094d\u091f\u094d\u0930 \u0915\u0947 \u092a\u094d\u0930\u092e\u0941\u0916 \u0915\u093e \u0915\u0939\u0928\u093e \u0939\u0948 \u0915\u093f \u0938\u0940\u0930\u093f\u092f\u093e \u092e\u0947\u0902 \u0915\u094b\u0908 \u0938\u0948\u0928\u094d\u092f \u0938\u092e\u093e\u0927\u093e\u0928 \u0928\u0939\u0940\u0902 \u0939\u0948\" mt . translate ( text_hi , source = dlt . lang . HINDI , target = dlt . lang . ENGLISH ) Above, you can see that dlt.lang contains variables representing each of the 50 available languages with auto-complete support. Alternatively, you can specify the language (e.g. \"Arabic\") or the language code (e.g. \"fr_XX\" for French): text_ar = \"\u0627\u0644\u0623\u0645\u064a\u0646 \u0627\u0644\u0639\u0627\u0645 \u0644\u0644\u0623\u0645\u0645 \u0627\u0644\u0645\u062a\u062d\u062f\u0629 \u064a\u0642\u0648\u0644 \u0625\u0646\u0647 \u0644\u0627 \u064a\u0648\u062c\u062f \u062d\u0644 \u0639\u0633\u0643\u0631\u064a \u0641\u064a \u0633\u0648\u0631\u064a\u0627.\" mt . translate ( text_ar , source = \"Arabic\" , target = \"fr_XX\" ) If you want to verify whether a language is available, you can check it: print ( mt . available_languages ()) # All languages that you can use print ( mt . available_codes ()) # Code corresponding to each language accepted print ( mt . get_lang_code_map ()) # Dictionary of lang -> code","title":"Quickstart"},{"location":"#usage","text":"","title":"Usage"},{"location":"#selecting-a-device","text":"When you load the model, you can specify the device: mt = dlt . TranslationModel ( device = \"auto\" ) By default, the value will be device=\"auto\" , which means it will use a GPU if possible. You can also explicitly set device=\"cpu\" or device=\"gpu\" , or some other strings accepted by torch.device() . In general, it is recommend to use a GPU if you want a reasonable processing time.","title":"Selecting a device"},{"location":"#loading-from-a-path","text":"By default, dlt.TranslationModel will download the model from the huggingface repo and cache it. However, you are free to load from a path: mt = dlt . TranslationModel ( \"/path/to/your/model/directory/\" ) Make sure that your tokenizer is also stored in the same directory if you use this approach.","title":"Loading from a path"},{"location":"#using-a-different-model","text":"You can also choose another model that has a similar format , e.g. mt = dlt . TranslationModel ( \"facebook/mbart-large-50-one-to-many-mmt\" ) Note that the available languages will change if you do this, so you will not be able to leverage dlt.lang or dlt.utils .","title":"Using a different model"},{"location":"#breaking-down-into-sentences","text":"It is not recommended to use extremely long texts as it takes more time to process. Instead, you can try to break them down into sentences with the help of nltk . First install the library with pip install nltk , then run: import nltk nltk . download ( \"punkt\" ) text = \"Mr. Smith went to his favorite cafe. There, he met his friend Dr. Doe.\" sents = nltk . tokenize . sent_tokenize ( text , \"english\" ) # don't use dlt.lang.ENGLISH \" \" . join ( mt . translate ( sents , source = dlt . lang . ENGLISH , target = dlt . lang . FRENCH ))","title":"Breaking down into sentences"},{"location":"#batch-size-and-verbosity-when-using-translate","text":"It's possible to set a batch size (i.e. the number of elements processed at once) for mt.translate and whether you want to see the progress bar or not: ... mt = dlt . TranslationModel () mt . translate ( text , source , target , batch_size = 32 , verbose = True ) If you set batch_size=None , it will compute the entire text at once rather than splitting into \"chunks\". We recommend lowering batch_size if you do not have a lot of RAM or VRAM and run into CUDA memory error. Set a higher value if you are using a high-end GPU and the VRAM is not fully utilized.","title":"Batch size and verbosity when using translate"},{"location":"#dltutils-module","text":"An alternative to mt.available_languages() is the dlt.utils module. You can use it to find out which languages and codes are available: print ( dlt . utils . available_languages ( 'mbart50' )) # All languages that you can use print ( dlt . utils . available_codes ( 'mbart50' )) # Code corresponding to each language accepted print ( dlt . utils . get_lang_code_map ( 'mbart50' )) # Dictionary of lang -> code","title":"dlt.utils module"},{"location":"#advanced","text":"The following section assumes you have knowledge of PyTorch and Huggingface Transformers.","title":"Advanced"},{"location":"#saving-and-loading","text":"If you wish to accelerate the loading time the translation model, you can use save_obj : mt = dlt . TranslationModel () mt . save_obj ( 'saved_model' ) # ... Then later you can reload it with load_obj : mt = dlt . TranslationModel . load_obj ( 'saved_model' ) # ... Warning: Only use this if you are certain the torch module saved in saved_model/weights.pt can be correctly loaded. Indeed, it is possible that the huggingface , torch or some other dependencies change between when you called save_obj and load_obj , and that might break your code. Thus, it is recommend to only run load_obj in the same environment/session as save_obj . Note this method might be deprecated in the future once there's no speed benefit in loading this way.","title":"Saving and loading"},{"location":"#interacting-with-underlying-model-and-tokenizer","text":"When initializing model , you can pass in arguments for the underlying BART model and tokenizer (which will respectively be passed to MBartForConditionalGeneration.from_pretrained and MBart50TokenizerFast.from_pretrained ): mt = dlt . TranslationModel ( model_options = dict ( state_dict =... , cache_dir =... , ... ), tokenizer_options = dict ( tokenizer_file =... , eos_token =... , ... ) ) You can also access the underlying transformers model and tokenizer : bart = mt . get_transformers_model () tokenizer = mt . get_tokenizer () See the huggingface docs for more information.","title":"Interacting with underlying model and tokenizer"},{"location":"#bart_modelgenerate-keyword-arguments","text":"When running mt.translate , you can also give a generation_options dictionary that is passed as keyword arguments to the underlying bart_model.generate() method: mt . translate ( text , source = dlt . lang . GERMAN , target = dlt . lang . SPANISH , generation_options = dict ( num_beams = 5 , max_length =... ) ) Learn more in the huggingface docs .","title":"bart_model.generate() keyword arguments"},{"location":"#acknowledgement","text":"dl-translate is built on top of Huggingface's implementation of multilingual BART finetuned on many-to-many translation of over 50 languages, which is documented here . The original paper was written by Tang et. al from Facebook AI Research; you can find it here and cite it using the following: @article{tang2020multilingual, title={Multilingual translation with extensible multilingual pretraining and finetuning}, author={Tang, Yuqing and Tran, Chau and Li, Xian and Chen, Peng-Jen and Goyal, Naman and Chaudhary, Vishrav and Gu, Jiatao and Fan, Angela}, journal={arXiv preprint arXiv:2008.00401}, year={2020} } dlt is a wrapper with useful utils to save you time. For huggingface's transformers , the following snippet is shown as an example: from transformers import MBartForConditionalGeneration , MBart50TokenizerFast article_hi = \"\u0938\u0902\u092f\u0941\u0915\u094d\u0924 \u0930\u093e\u0937\u094d\u091f\u094d\u0930 \u0915\u0947 \u092a\u094d\u0930\u092e\u0941\u0916 \u0915\u093e \u0915\u0939\u0928\u093e \u0939\u0948 \u0915\u093f \u0938\u0940\u0930\u093f\u092f\u093e \u092e\u0947\u0902 \u0915\u094b\u0908 \u0938\u0948\u0928\u094d\u092f \u0938\u092e\u093e\u0927\u093e\u0928 \u0928\u0939\u0940\u0902 \u0939\u0948\" article_ar = \"\u0627\u0644\u0623\u0645\u064a\u0646 \u0627\u0644\u0639\u0627\u0645 \u0644\u0644\u0623\u0645\u0645 \u0627\u0644\u0645\u062a\u062d\u062f\u0629 \u064a\u0642\u0648\u0644 \u0625\u0646\u0647 \u0644\u0627 \u064a\u0648\u062c\u062f \u062d\u0644 \u0639\u0633\u0643\u0631\u064a \u0641\u064a \u0633\u0648\u0631\u064a\u0627.\" model = MBartForConditionalGeneration . from_pretrained ( \"facebook/mbart-large-50-many-to-many-mmt\" ) tokenizer = MBart50TokenizerFast . from_pretrained ( \"facebook/mbart-large-50-many-to-many-mmt\" ) # translate Hindi to French tokenizer . src_lang = \"hi_IN\" encoded_hi = tokenizer ( article_hi , return_tensors = \"pt\" ) generated_tokens = model . generate ( ** encoded_hi , forced_bos_token_id = tokenizer . lang_code_to_id [ \"fr_XX\" ]) tokenizer . batch_decode ( generated_tokens , skip_special_tokens = True ) # => \"Le chef de l 'ONU affirme qu 'il n 'y a pas de solution militaire en Syria.\" # translate Arabic to English tokenizer . src_lang = \"ar_AR\" encoded_ar = tokenizer ( article_ar , return_tensors = \"pt\" ) generated_tokens = model . generate ( ** encoded_ar , forced_bos_token_id = tokenizer . lang_code_to_id [ \"en_XX\" ]) tokenizer . batch_decode ( generated_tokens , skip_special_tokens = True ) # => \"The Secretary-General of the United Nations says there is no military solution in Syria.\" With dlt , you can run: import dl_translate as dlt article_hi = \"\u0938\u0902\u092f\u0941\u0915\u094d\u0924 \u0930\u093e\u0937\u094d\u091f\u094d\u0930 \u0915\u0947 \u092a\u094d\u0930\u092e\u0941\u0916 \u0915\u093e \u0915\u0939\u0928\u093e \u0939\u0948 \u0915\u093f \u0938\u0940\u0930\u093f\u092f\u093e \u092e\u0947\u0902 \u0915\u094b\u0908 \u0938\u0948\u0928\u094d\u092f \u0938\u092e\u093e\u0927\u093e\u0928 \u0928\u0939\u0940\u0902 \u0939\u0948\" article_ar = \"\u0627\u0644\u0623\u0645\u064a\u0646 \u0627\u0644\u0639\u0627\u0645 \u0644\u0644\u0623\u0645\u0645 \u0627\u0644\u0645\u062a\u062d\u062f\u0629 \u064a\u0642\u0648\u0644 \u0625\u0646\u0647 \u0644\u0627 \u064a\u0648\u062c\u062f \u062d\u0644 \u0639\u0633\u0643\u0631\u064a \u0641\u064a \u0633\u0648\u0631\u064a\u0627.\" mt = dlt . TranslationModel () translated_fr = mt . translate ( article_hi , source = dlt . lang . HINDI , target = dlt . lang . FRENCH ) translated_en = mt . translate ( article_ar , source = dlt . lang . ARABIC , target = dlt . lang . ENGLISH ) Notice you don't have to think about tokenizers, condition generation, pretrained models, and regional codes; you can just tell the model what to translate! If you are experienced with huggingface 's ecosystem, then you should be familiar enough with the example above that you wouldn't need this library. However, if you've never heard of huggingface or mBART, then I hope using this library will give you enough motivation to learn more about them :)","title":"Acknowledgement"},{"location":"contributing/","text":"Contributions If you wish to contribute to the project, please do the following: 1. Verify if there's an existing similar issue. 2. If no issue exists, create it. 3. Once the contribution has been discussed inside the issue, fork this repo. 4. Before modifying any code, make sure to read the sections below. 5. Once you are done with your contribution, start a PR and tag a codeowner. Setup To set up the development environment, clone the repo: git clone https://github.com/xhlulu/dl-translate cd dl-translate Create a new venv and install the dev dependencies python -m venv venv source venv/bin/activate pip install -e . [ dev ] Code linting To ensure consistent and readable code, we use black . To run: python black . Running tests To run all the tests: python -m pytest tests For quick tests, run: python -m pytest tests/fast Documentation To re-generate the documentation after the source code was modified: python scripts/render_references.py To run the docs locally, run: mkdocs serve -t material Once ready, you can build it: mkdocs build -t material Or release it on GitHub Pages: mkdocs gh-deploy -t material","title":"Contributions"},{"location":"contributing/#contributions","text":"If you wish to contribute to the project, please do the following: 1. Verify if there's an existing similar issue. 2. If no issue exists, create it. 3. Once the contribution has been discussed inside the issue, fork this repo. 4. Before modifying any code, make sure to read the sections below. 5. Once you are done with your contribution, start a PR and tag a codeowner.","title":"Contributions"},{"location":"contributing/#setup","text":"To set up the development environment, clone the repo: git clone https://github.com/xhlulu/dl-translate cd dl-translate Create a new venv and install the dev dependencies python -m venv venv source venv/bin/activate pip install -e . [ dev ]","title":"Setup"},{"location":"contributing/#code-linting","text":"To ensure consistent and readable code, we use black . To run: python black .","title":"Code linting"},{"location":"contributing/#running-tests","text":"To run all the tests: python -m pytest tests For quick tests, run: python -m pytest tests/fast","title":"Running tests"},{"location":"contributing/#documentation","text":"To re-generate the documentation after the source code was modified: python scripts/render_references.py To run the docs locally, run: mkdocs serve -t material Once ready, you can build it: mkdocs build -t material Or release it on GitHub Pages: mkdocs gh-deploy -t material","title":"Documentation"},{"location":"references/","text":"API Reference dlt.TranslationModel init dlt . TranslationModel . __init__ ( self , model_or_path : str = 'facebook/mbart-large-50-many-to-many-mmt' , tokenizer_path : str = None , device : str = 'auto' , model_options : dict = None , tokenizer_options : dict = None ) Instantiates a multilingual transformer model for translation. Parameter Type Default Description model_or_path str facebook/mbart-large-50-many-to-many-mmt The path or the name of the model. Equivalent to the first argument of AutoModel.from_pretrained(). device str auto \"cpu\", \"gpu\" or \"auto\". If it's set to \"auto\", will try to select a GPU when available or else fallback to CPU. tokenizer_path str optional The path to the tokenizer, only if it is different from model_or_path ; otherwise, leave it as None . model_options dict optional The keyword arguments passed to the transformer model, which is a mBART-Large for condition generation. tokenizer_options dict optional The keyword arguments passed to the tokenizer model, which is a mBART-50 Fast Tokenizer. translate dlt . TranslationModel . translate ( self , text : Union [ str , List [ str ]], source : str , target : str , batch_size : int = 32 , verbose : bool = False , generation_options : dict = None ) -> Union [ str , List [ str ]] Translates a string or a list of strings from a source to a target language. Parameter Type Default Description text Union[str, List[str]] required The content you want to translate. source str required The language of the original text. target str required The language of the translated text. batch_size int 32 The number of samples to load at once. A smaller value is preferred if you do not have a lot of (video) RAM. If set to None , it will process everything at once. verbose bool False Whether to display the progress bar for every batch processed. generation_options dict optional The keyword arguments passed to bart_model.generate(), where bart_model is the underlying transformers model. Tip: run print(dlt.utils.available_languages()) to see what's available. get_transformers_model dlt . TranslationModel . get_transformers_model ( self ) -> transformers . models . mbart . modeling_mbart . MBartForConditionalGeneration Retrieve the underlying mBART transformer model. get_tokenizer dlt . TranslationModel . get_tokenizer ( self ) -> transformers . models . mbart . tokenization_mbart50_fast . MBart50TokenizerFast Retrieve the mBART huggingface tokenizer. available_codes dlt . TranslationModel . available_codes ( self ) -> List [ str ] Returns all the available codes for a given dlt.TranslationModel instance. available_languages dlt . TranslationModel . available_languages ( self ) -> List [ str ] Returns all the available languages for a given dlt.TranslationModel instance. get_lang_code_map dlt . TranslationModel . get_lang_code_map ( self ) -> Dict [ str , str ] Returns the language -> codes dictionary for a given dlt.TranslationModel instance. save_obj dlt . TranslationModel . save_obj ( self , path : str = 'saved_model' ) -> None Saves your model as a torch object and save your tokenizer. Parameter Type Default Description path str saved_model The directory where you want to save your model and tokenizer load_obj dlt . TranslationModel . load_obj ( path : str = 'saved_model' , ** kwargs ) Initialize dlt.TranslationModel from the torch object and tokenizer saved with dlt.TranslationModel.save_obj Parameter Type Default Description path str saved_model The directory where your torch model and tokenizer are stored dlt.utils get_lang_code_map dlt . utils . get_lang_code_map ( weights : str = 'mbart50' ) -> Dict [ str , str ] Get a dictionary mapping a language -> code for a given model. The code will depend on the model you choose. Parameter Type Default Description weights str mbart50 The name of the model you are using. For example, \"mbart50\" is the multilingual BART Large with 50 languages available to use. available_codes dlt . utils . available_codes ( weights : str = 'mbart50' ) -> List [ str ] Get all the codes available for a given model. The code format will depend on the model you select. Parameter Type Default Description weights str mbart50 The name of the model you are using. For example, \"mbart50\" is the multilingual BART Large with 50 codes available to use. available_languages dlt . utils . available_languages ( weights : str = 'mbart50' ) -> List [ str ] Get all the languages available for a given model. Parameter Type Default Description weights str mbart50 The name of the model you are using. For example, \"mbart50\" is the multilingual BART Large with 50 languages available to use.","title":"API Reference"},{"location":"references/#api-reference","text":"","title":"API Reference"},{"location":"references/#dlttranslationmodel","text":"","title":"dlt.TranslationModel"},{"location":"references/#init","text":"dlt . TranslationModel . __init__ ( self , model_or_path : str = 'facebook/mbart-large-50-many-to-many-mmt' , tokenizer_path : str = None , device : str = 'auto' , model_options : dict = None , tokenizer_options : dict = None ) Instantiates a multilingual transformer model for translation. Parameter Type Default Description model_or_path str facebook/mbart-large-50-many-to-many-mmt The path or the name of the model. Equivalent to the first argument of AutoModel.from_pretrained(). device str auto \"cpu\", \"gpu\" or \"auto\". If it's set to \"auto\", will try to select a GPU when available or else fallback to CPU. tokenizer_path str optional The path to the tokenizer, only if it is different from model_or_path ; otherwise, leave it as None . model_options dict optional The keyword arguments passed to the transformer model, which is a mBART-Large for condition generation. tokenizer_options dict optional The keyword arguments passed to the tokenizer model, which is a mBART-50 Fast Tokenizer.","title":"init"},{"location":"references/#translate","text":"dlt . TranslationModel . translate ( self , text : Union [ str , List [ str ]], source : str , target : str , batch_size : int = 32 , verbose : bool = False , generation_options : dict = None ) -> Union [ str , List [ str ]] Translates a string or a list of strings from a source to a target language. Parameter Type Default Description text Union[str, List[str]] required The content you want to translate. source str required The language of the original text. target str required The language of the translated text. batch_size int 32 The number of samples to load at once. A smaller value is preferred if you do not have a lot of (video) RAM. If set to None , it will process everything at once. verbose bool False Whether to display the progress bar for every batch processed. generation_options dict optional The keyword arguments passed to bart_model.generate(), where bart_model is the underlying transformers model. Tip: run print(dlt.utils.available_languages()) to see what's available.","title":"translate"},{"location":"references/#get_transformers_model","text":"dlt . TranslationModel . get_transformers_model ( self ) -> transformers . models . mbart . modeling_mbart . MBartForConditionalGeneration Retrieve the underlying mBART transformer model.","title":"get_transformers_model"},{"location":"references/#get_tokenizer","text":"dlt . TranslationModel . get_tokenizer ( self ) -> transformers . models . mbart . tokenization_mbart50_fast . MBart50TokenizerFast Retrieve the mBART huggingface tokenizer.","title":"get_tokenizer"},{"location":"references/#available_codes","text":"dlt . TranslationModel . available_codes ( self ) -> List [ str ] Returns all the available codes for a given dlt.TranslationModel instance.","title":"available_codes"},{"location":"references/#available_languages","text":"dlt . TranslationModel . available_languages ( self ) -> List [ str ] Returns all the available languages for a given dlt.TranslationModel instance.","title":"available_languages"},{"location":"references/#get_lang_code_map","text":"dlt . TranslationModel . get_lang_code_map ( self ) -> Dict [ str , str ] Returns the language -> codes dictionary for a given dlt.TranslationModel instance.","title":"get_lang_code_map"},{"location":"references/#save_obj","text":"dlt . TranslationModel . save_obj ( self , path : str = 'saved_model' ) -> None Saves your model as a torch object and save your tokenizer. Parameter Type Default Description path str saved_model The directory where you want to save your model and tokenizer","title":"save_obj"},{"location":"references/#load_obj","text":"dlt . TranslationModel . load_obj ( path : str = 'saved_model' , ** kwargs ) Initialize dlt.TranslationModel from the torch object and tokenizer saved with dlt.TranslationModel.save_obj Parameter Type Default Description path str saved_model The directory where your torch model and tokenizer are stored","title":"load_obj"},{"location":"references/#dltutils","text":"","title":"dlt.utils"},{"location":"references/#get_lang_code_map_1","text":"dlt . utils . get_lang_code_map ( weights : str = 'mbart50' ) -> Dict [ str , str ] Get a dictionary mapping a language -> code for a given model. The code will depend on the model you choose. Parameter Type Default Description weights str mbart50 The name of the model you are using. For example, \"mbart50\" is the multilingual BART Large with 50 languages available to use.","title":"get_lang_code_map"},{"location":"references/#available_codes_1","text":"dlt . utils . available_codes ( weights : str = 'mbart50' ) -> List [ str ] Get all the codes available for a given model. The code format will depend on the model you select. Parameter Type Default Description weights str mbart50 The name of the model you are using. For example, \"mbart50\" is the multilingual BART Large with 50 codes available to use.","title":"available_codes"},{"location":"references/#available_languages_1","text":"dlt . utils . available_languages ( weights : str = 'mbart50' ) -> List [ str ] Get all the languages available for a given model. Parameter Type Default Description weights str mbart50 The name of the model you are using. For example, \"mbart50\" is the multilingual BART Large with 50 languages available to use.","title":"available_languages"}]}